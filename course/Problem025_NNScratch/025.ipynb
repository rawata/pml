{
 "cells": [
  {
   "source": [
    "## TBD Code reading, Below is a from-scratch  3 layer neural network. Main training code is in the *fit* method "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (relu Function) x if x > 0 else x\n",
    "\n",
    "    :param Z: input tensor\n",
    "    :return:  Relu output\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid Function\n",
    "    :param Z: input tensor\n",
    "    :return: Sigmoid output\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Derivative of relu function\n",
    "\n",
    "    :param dA:  How cost changes with activation ( dJ_by_dA )\n",
    "    :param Z:\n",
    " \n",
    "    :return:  How cost changes with weighted sum (Z) ( dJ_by_dZ )\n",
    "    \"\"\"\n",
    "\n",
    "    dZ = np.array(dA, copy=True) \n",
    "\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid function\n",
    "\n",
    "    :param dA: How cost changes with activation ( dJ_by_dA )\n",
    "    :param Z:\n",
    "    \n",
    "    :return: How cost changes with weighted sum (Z) ( dJ_by_dZ )\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "\n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_x, n_h1, n_h2, n_y):\n",
    "    \"\"\"\n",
    "\n",
    "    Initialize weights tensor\n",
    "\n",
    "    :param n_x: size of the input layer\n",
    "    :param n_h1: size of the hidden layer 1\n",
    "    :param n_h2: size of the hidden layer 2\n",
    "    :param n_y: size of the output layer\n",
    "\n",
    "    :return:     weights -- python dictionary containing initialized weights:\n",
    "                    W1 -- weight matrix of shape (n_h1, n_x)\n",
    "                    b1 -- bias vector of shape (n_h1, 1)\n",
    "                    W2 -- weight matrix of shape (n_h2, n_h1)\n",
    "                    b2 -- bias vector of shape (n_h2, 1)\n",
    "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
    "                    b3 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h1, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h1, 1))\n",
    "    W2 = np.random.randn(n_h2, n_h1) * 0.01\n",
    "    b2 = np.zeros((n_h2, 1))\n",
    "    W3 = np.random.randn(n_y, n_h2) * 0.01\n",
    "    b3 = np.zeros((n_y, 1))\n",
    "\n",
    "    weights = { \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 }\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward(A, W, b, activation_function):\n",
    "    \"\"\"\n",
    "    Step forward one layer in a deep neural network\n",
    "\n",
    "    :param A: Input tensor for the layer\n",
    "    :param W: Weight tensor for the layer\n",
    "    :param b: Bias tensor for the layer\n",
    "    :param activation_function: Activation function to be applied to the layer\n",
    "\n",
    "    :return: Output Activation tensor and Weighted sum\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.matmul(W, A) + b\n",
    "    A_next = activation_function(Z)\n",
    "\n",
    "    return A_next, Z\n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "\n",
    "    Cross Entropy Error (Cost) between Y-Hat and Y  (Y-Hat is activation of Lth Layer hence AL)\n",
    "\n",
    "    :param AL: Activation of Lth Layer i.e Y-Hat\n",
    "    :param Y: Actual Labels (Ground Truth)\n",
    "\n",
    "    :return: Cost (How close are we?)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -(np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))) / m\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def step_backward(dA, Z, A_prev, W, b, activation_function_backward):\n",
    "    \"\"\"\n",
    "    Step back one layer in deep neural network,\n",
    "    i.e Find out how much adjustment is needed in weights in this layer\n",
    "\n",
    "    :param dA: How Final cost changes with Activation of this layer ( dJ_by_dA )\n",
    "    :param Z: Weighted Sum from this layer\n",
    "    :param A_prev: Incoming activations to this layer\n",
    "    :param W: Weights of this layer\n",
    "    :param b: Bias of this layer\n",
    "    :param activation_function_backward:\n",
    "    \n",
    "    :return:\n",
    "        dA_prev : How cost changes with activation from prev layer ( dJ_by_dA_prev), need this for further back propagation of cost to previous layer\n",
    "        dW: How cost changes with weights of this layer (our prize). (dJ_by_dW)\n",
    "        db: How cost changes with weights of this layer (dJ_by_db)\n",
    "    \"\"\"\n",
    "\n",
    "    dZ = activation_function_backward(dA, Z)\n",
    "\n",
    "    N = A_prev.shape[1]\n",
    "\n",
    "    dW = np.matmul(dZ, A_prev.transpose()) / N\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / N\n",
    "    dA_prev = np.matmul(W.transpose(), dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def update_weights(weights, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update weights across all layers\n",
    "    After our back-propogation we know how much weight & bias adjustment is needed\n",
    "    in each layer, This function performs that update.\n",
    "\n",
    "    :param weights: Original weights\n",
    "    :param grads: Changes that needs to be made to weights (dW and db s)\n",
    "    :param learning_rate: learning rate of algorithm. (a step length in gradient descent)\n",
    "\n",
    "    :return: updated weights\n",
    "    \"\"\"\n",
    "    L = len(weights)//2    # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        weights[\"W\" + str(l + 1)] = weights[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        weights[\"b\" + str(l + 1)] = weights[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "source": [
    "## Fit (Training of Neural network using back propagation and gradient descent )\n",
    "<img src=\"../../img/nn_training_loop.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fit(X, Y, layers_dims, learning_rate=0.0075, num_iterations=10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Trains a 3 layer neural network\n",
    "\n",
    "    :param X: input features\n",
    "    :param Y: output labels\n",
    "    :param layers_dims: number of nodes in each layer ( inputs, number of neurons in each hidden layer, output)\n",
    "    :param learning_rate: \n",
    "    :param num_iterations:\n",
    "    :param print_cost: \n",
    "\n",
    "    :return: Weights and Bias of the trained network. \n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []  # to keep track of the cost\n",
    "    m = X.shape[1]  # number of examples\n",
    "    \n",
    "    (n_x, n_h1, n_h2, n_y) = layers_dims\n",
    "\n",
    "    weights = init_weights(n_x, n_h1, n_h2, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: (LINEAR -> RELU) * 2 -> LINEAR -> SIGMOID.\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from weights\n",
    "        W1 = weights[\"W1\"]\n",
    "        b1 = weights[\"b1\"]\n",
    "        W2 = weights[\"W2\"]\n",
    "        b2 = weights[\"b2\"]\n",
    "        W3 = weights[\"W3\"]\n",
    "        b3 = weights[\"b3\"]\n",
    "\n",
    "\n",
    "        A1, Z1  = step_forward(X, W1, b1, relu)\n",
    "        A2, Z2  = step_forward(A1, W2, b2, relu)\n",
    "        A3, Z3  = step_forward(A2, W3, b3, sigmoid)\n",
    "\n",
    "        cost = compute_cost(A3, Y)\n",
    "\n",
    "        # Initializing backward propagation\n",
    "        dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3))\n",
    "\n",
    "        # gradient descent through back propagation\n",
    "        dA2, dW3, db3 = step_backward(dA3, Z3, A2, W3, b3, sigmoid_backward)\n",
    "        dA1, dW2, db2 = step_backward(dA2, Z2, A1, W2, b2, relu_backward)\n",
    "        dA0, dW1, db1 = step_backward(dA1, Z1, X, W1, b1,  relu_backward)\n",
    "\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        grads['dW3'] = dW3\n",
    "        grads['db3'] = db3\n",
    "\n",
    "        # Update weights.\n",
    "        weights = update_weights(weights, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "source": [
    "## Predict and Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    \"\"\"\n",
    "    Given weights of deep neural network, predict the output for X\n",
    "\n",
    "    :param X: input tensor\n",
    "    :param weights: Weights and Bias of the network\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    W1 = weights[\"W1\"]\n",
    "    b1 = weights[\"b1\"]\n",
    "    W2 = weights[\"W2\"]\n",
    "    b2 = weights[\"b2\"]\n",
    "    W3 = weights[\"W3\"]\n",
    "    b3 = weights[\"b3\"]\n",
    "\n",
    "    A1, Z1 = step_forward(X, W1, b1, relu)\n",
    "    A2, Z2 = step_forward(A1, W2, b2, relu)\n",
    "    A3, Z3 = step_forward(A2, W3, b3, sigmoid)\n",
    "    A3 = np.around(A3)\n",
    "    return A3\n",
    "\n",
    "\n",
    "def compute_accuracy(X, Y, weights):\n",
    "    \"\"\"\n",
    "    Compute predictions for X given the weights and then find out how accurate are the predictions\n",
    "    by comparing with y\n",
    "\n",
    "    :param X: Input Tensors to be predicted\n",
    "    :param Y: Ground Truth Labels\n",
    "    :param weights: Weight (Model)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    N = X.shape[1]\n",
    "    A3 = predict(X, weights)\n",
    "    res = A3 == Y\n",
    "    accuracy = np.sum(res.all(axis=0)) / N\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    return accuracy\n"
   ]
  },
  {
   "source": [
    "## Train on sklearn MNIST data \n",
    "input = 8X8 pixels\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "\n",
    "X = digits.data\n",
    "\n",
    "# One hot encoding of target (Y)\n",
    "Y = np_utils.to_categorical(digits.target, 10)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Transposing the input/output data as implementation expects them to be\n",
    "# In Library implementations its usually the weights which are transposed\n",
    "\n",
    "X_train = np.transpose(X_train)\n",
    "X_test = np.transpose(X_test)\n",
    "Y_train = np.transpose(Y_train)\n",
    "Y_test = np.transpose(Y_test)\n",
    "\n",
    "\n",
    "n_x = 64\n",
    "n_h1 = 40\n",
    "n_h2 = 20\n",
    "n_y = 10\n",
    "layers_dims = (n_x, n_h1, n_h2, n_y)\n",
    "\n",
    "weights = fit(X_train, Y_train, layers_dims = (n_x, n_h1, n_h2, n_y), num_iterations = 10000, print_cost=True)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Predict and compute accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = compute_accuracy(X_train, Y_train, weights)\n",
    "accuracy_test = compute_accuracy(X_test, Y_test, weights)\n"
   ]
  },
  {
   "source": [
    "## TBD: Show some images for which the model makes errors\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TBD: Generalize the Fit function to accept any number of layers.\n",
    "Currently fit is hardcoded for a 3 Layer neural network, with Relu Activation for L1 and L2 and Sigmoid activation for L3. Generalize the fit  to accept any layer and activation functions. You can use following sequence of steps to achieve this.\n",
    "\n",
    "### TBD 1.  Change layer_dims to layer_confs, previously it was just a list of numbers but now it will be a list of tuples (number_of_nodes, activation_function). E.g layer_confs for existing NN will be. \\[ (64, None), (40, relu), (20, relu), (10, sigmoid) \\] \n",
    "\n",
    "### TBD 2.  Modify init_weights function to initilaize a general weights setting using layer_confs\n",
    "\n",
    "### TBD 3.  Modify fit function to use layer_confs to loop to go forward and backward.\n",
    "\n",
    "### TBD 4.  Use this general function to test the existing 3 layer configuration and see if you get same accuracy results\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('pml': venv)",
   "metadata": {
    "interpreter": {
     "hash": "a4c9474aacc61cf72d0f1c29f4a339e5d6b2171c287541cfd684cf058783219b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}