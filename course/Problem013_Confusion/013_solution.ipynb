{
 "cells": [
  {
   "source": [
    "# How to evaluate following systems?\n",
    "\n",
    "## Credit Approval (Yes/No)\n",
    "## Covid detection (Positive/Negative)\n",
    "## Court guilty judgement (Guilty/Not Guilty)\n",
    "## Spam detection (Spam/Ham)\n",
    "## Anomaly detection (Anomaly/Normal)\n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Changed data slightly from original problem, as that was sort of edge case.\n",
    "# Feel free to apply this solution on the original data as well \n",
    "\n",
    "# Some classification process under study generated following labels \n",
    "Y =  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "# 2 classification models trying to approximate above process produced following end predictions\n",
    "Y_hat1 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "Y_hat2 = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "#TBD Which is the better model\n",
    "\n",
    "#Solution:\n",
    "# We have 2 output classes 0 and 1,\n",
    "# from accuracy perspective Model1 looks better model, but if you notice, output  \n",
    "# class distribution is very skewed (imbalanced), 1 is a rare class. So even a dumb model which just \n",
    "# produces zeros will have good accuracy without trying anything. Model2 overall has got \n",
    "# more wrong but it got all of the rare class right, so in some scenarios it might be a better model \n",
    "# In case of skewed classes, accuracy is not a good metrics to rate model as we will \n",
    "# see below precision/recall and f1 score are more indicative of model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy of model1 = 0.9. 90.0% accurate\naccuracy of model2 = 0.8. 80.0% accurate\n"
     ]
    }
   ],
   "source": [
    "# TBD: Compute accuracy of the models\n",
    "\n",
    "#Solution\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy1 = accuracy_score(Y, Y_hat1)\n",
    "accuracy2 = accuracy_score(Y, Y_hat2)\n",
    "\n",
    "print(f\"accuracy of model1 = {accuracy1}. {accuracy1*100}% accurate\")\n",
    "print(f\"accuracy of model2 = {accuracy2}. {accuracy2*100}% accurate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "confusion1 = \n[[1 0]\n [1 8]]\nconfusion2 = \n[[2 2]\n [0 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# TBD: Compute confusion matrix for the models\n",
    "\n",
    "#Solution\n",
    "\n",
    "'''\n",
    "Positive/Negative refer to class\n",
    "    1 is Positive Class (Usually rare class should be kept positive as metrics like precision, recall are defined from perspective of positive class)\n",
    "    0 is Negative Class\n",
    "\n",
    "True Positives (TP)  = Classified Positive and are Correct\n",
    "False Positives (FP) = Classified Positive and are Incorrect\n",
    "False Negatives (FN) = Classified Negative and are Incorrect\n",
    "True Negatives (TN)  = Classified Negative and are Correct\n",
    "\n",
    "True/False refers to classification\n",
    "    True means classified correctly\n",
    "    False means classified incorrectly\n",
    "\n",
    "+-------------------------------------------------+\n",
    "|                 Confusion Matrix                |\n",
    "+------------+-----------------+------------------+\n",
    "|            | Actual Positive | Actual Negative  |\n",
    "+------------+-----------------+------------------+\n",
    "| Predicted  | True Positive   | False Positive   |\n",
    "| Positive   |                 |                  |\n",
    "+------------+-----------------+------------------+\n",
    "| Predicted  | False Negative  | True Negative    |\n",
    "| Negative   |                 |                  |\n",
    "+------------+-----------------+------------------+\n",
    "\n",
    "'''\n",
    "# \n",
    "# Y      = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# Y_hat1 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "confusion1 = np.array([\n",
    "    [1, 0],\n",
    "    [1, 8]\n",
    "])\n",
    "\n",
    "# \n",
    "# Y      = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# Y_hat2 = [1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "confusion2 = np.array([\n",
    "    [2, 2],\n",
    "    [0, 6]\n",
    "])\n",
    "\n",
    "print(f\"confusion1 = \\n{confusion1}\")\n",
    "print(f\"confusion2 = \\n{confusion2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision1 = 1.0\nprecision2 = 0.5\nrecall1 = 0.5\nrecall2 = 1.0\nF1 score of model1 = 0.6666666666666666\nF1 score of model2 = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# TBD: Compute precision, recall and f1 score for the models\n",
    "\n",
    "#Solution (Precision, Recall, and F1 score Conceptually)\n",
    "\n",
    "# Precision = TP/(TP + FP)\n",
    "# Precision means what fraction of class we are classifying positive is actually positive\n",
    "precision1 = 1/(1 + 0)\n",
    "print(f\"precision1 = {precision1}\")\n",
    "precision2 = 2/(2 + 2)\n",
    "print(f\"precision2 = {precision2}\")\n",
    "\n",
    "# Recall = TP/(TP + FN)\n",
    "# Recall means what fraction of total positive class we are classifying as positive\n",
    "recall1 = 1/(1+1)\n",
    "print(f\"recall1 = {recall1}\")\n",
    "recall2 = 2/(2+0)\n",
    "print(f\"recall2 = {recall2}\")\n",
    "\n",
    "\n",
    "# Usually ML systems tradeoff between precision and recall, higher precision\n",
    "# may lead to lower recall and vise versa. for e.g in a Serious disease detector\n",
    "# we may favour recall over precision as we did not want to miss any case.\n",
    "# Some judicial systems heavily tilt towards precision at the cost of recall\n",
    "# It doesnt matter if 100 guilty are acquitted, but not one innocent should be punished.\n",
    "# Similarly if designing a anomaly detection system you would favour recall for mission\n",
    "# critical system, but for normal system you might favour precision to avoid waking\n",
    "# engineers at night\n",
    "\n",
    "\n",
    "# F1 score, Usually in case of skewed classes, metrics like accuracy are not\n",
    "# trustworthy, precision, recall serve the purpose but they are 2 metrics\n",
    "# and usually for evaluation of ML systems its reccommended to have single value metrics\n",
    "# F1 combines both precision and recall, its a harmonic mean of precision and recall\n",
    "\n",
    "# f1 = 2 * precision * recall /(precision + recall)\n",
    "\n",
    "def f1(precision, recall):\n",
    "    return 2*precision* recall/(precision + recall)\n",
    "\n",
    "f1_1 = f1(precision1, recall1)\n",
    "print(f\"F1 score of model1 = {f1_1}\")\n",
    "f1_2 = f1(precision2, recall2)\n",
    "print(f\"F1 score of model2 = {f1_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision1 = 1.0\nprecision2 = 0.5\nrecall1 = 0.5\nrecall2 = 1.0\nF1 score of model1 = 0.6666666666666666\nF1 score of model2 = 0.6666666666666666\nmodel1 precision with 0 as positive class = 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Solution: using sklearn\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision1 = precision_score(Y, Y_hat1) \n",
    "precision2 = precision_score(Y, Y_hat2)\n",
    "\n",
    "print(f\"precision1 = {precision1}\")\n",
    "print(f\"precision2 = {precision2}\")\n",
    "\n",
    "recall1 = recall_score(Y, Y_hat1)\n",
    "recall2 = recall_score(Y, Y_hat2)\n",
    "\n",
    "print(f\"recall1 = {recall1}\")\n",
    "print(f\"recall2 = {recall2}\")\n",
    "\n",
    "f1_1 = f1_score(Y, Y_hat1)\n",
    "f1_2 = f1_score(Y, Y_hat2)\n",
    "\n",
    "print(f\"F1 score of model1 = {f1_1}\")\n",
    "print(f\"F1 score of model2 = {f1_2}\")\n",
    "\n",
    "# Precision, Recall and F1 are defined around notion of positive class\n",
    "# 1 is default positive class, but if you want another label to be positive class\n",
    "# you can override it by specifying pos_label argument\n",
    "\n",
    "model1_precision_wrt_0_as_positive_class = precision_score(Y, Y_hat1, pos_label=0)\n",
    "print(f\"model1 precision with 0 as positive class = {model1_precision_wrt_0_as_positive_class}\")\n"
   ]
  },
  {
   "source": [
    "## Precision/Recall Gold Mining Analogy\n",
    "\n",
    "<img src=\"../../img/PrecisionRecallGoldMiningAnalogy.png\" width=\"900\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "confusion1_sklearn = \n[[1 1]\n [0 8]]\nconfusion2_skelarn = \n[[2 0]\n [2 6]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Note: Confusion matrix using sklearn has Actual Class in Rows and Predicted class in columns (i.e axes are other way round)\n",
    "'''\n",
    "+-----------------------------------------------------+\n",
    "|              Confusion Matrix (sklearn)             |\n",
    "+----------+--------------------+---------------------+\n",
    "|          | Predicted Positive | Predicted Negative  |\n",
    "+----------+--------------------+---------------------+\n",
    "| Actual   | True Positive      | False Negative      |\n",
    "| Positive |                    |                     |\n",
    "+----------+--------------------+---------------------+\n",
    "| Actual   | False Positive     | True Negative       |\n",
    "| Negative |                    |                     |\n",
    "+----------+--------------------+---------------------+\n",
    "'''\n",
    "#Positive and Negative Labels can be specified in labels arguments \n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion1_sklearn = confusion_matrix(Y, Y_hat1, labels = [1, 0]) #Order in which we specify labels will decide which will be positive class, [1, 0] means 1 is positive class, 0 is negative class\n",
    "confusion2_sklearn = confusion_matrix(Y, Y_hat2, labels = [1, 0])\n",
    "print(f\"confusion1_sklearn = \\n{confusion1_sklearn}\")\n",
    "print(f\"confusion2_skelarn = \\n{confusion2_sklearn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('pml': venv)",
   "metadata": {
    "interpreter": {
     "hash": "a4c9474aacc61cf72d0f1c29f4a339e5d6b2171c287541cfd684cf058783219b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}